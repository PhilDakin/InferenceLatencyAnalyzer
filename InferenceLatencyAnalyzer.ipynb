{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j368EA-EJ1fG",
        "outputId": "bf5c13c8-d22b-43c3-e891-0777c06cee4e"
      },
      "outputs": [],
      "source": [
        "# Written for Google Colab.\n",
        "\n",
        "!pip install -q -U \\\n",
        "  git+https://github.com/huggingface/transformers.git \\\n",
        "  git+https://github.com/huggingface/accelerate.git\n",
        "\n",
        "!pip install -q \\\n",
        "  datasets \\\n",
        "  bitsandbytes \\\n",
        "  einops \\\n",
        "  wandb \\\n",
        "  contexttimer \\\n",
        "  ray \\\n",
        "  pandas \\\n",
        "  tenacity \\\n",
        "  black[jupyter] \\\n",
        "  openai\n",
        "\n",
        "# -DLLAMA_CUBLAS=on build will fail if no GPU present.\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-XV8zq_RQAs",
        "outputId": "8e02f4c7-8570-4ed8-c02e-02c815923f96"
      },
      "outputs": [],
      "source": [
        "# Direct HF cache to Drive location s.t. models persist across instances.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/drive\")\n",
        "os.environ[\"HF_HOME\"] = \"/drive/MyDrive/HFCache\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJNxUx2BGk9t"
      },
      "outputs": [],
      "source": [
        "# Code formatting (https://stackoverflow.com/questions/63076002/code-formatter-like-nb-black-for-google-colab).\n",
        "# !black \"/drive/MyDrive/Colab Notebooks/InferenceLatencyAnalyzer.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55iDr_rSKzAI"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    Pipeline,\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "import contexttimer\n",
        "import torch\n",
        "import ray\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Callable, List, Tuple\n",
        "from collections.abc import Iterator\n",
        "\n",
        "\n",
        "def evaluate_model_latency(\n",
        "    model: Callable[[str, int], str], repeat: int, prompt: str, max_seq_length: int\n",
        ") -> Iterator[Tuple[str, float]]:\n",
        "    for _ in range(repeat):\n",
        "        with contexttimer.Timer() as timer:\n",
        "            output = model(prompt, max_length=max_seq_length)\n",
        "        yield (output, timer.elapsed)\n",
        "\n",
        "\n",
        "@ray.remote(num_gpus=1, max_calls=1)\n",
        "def _load_and_evaluate_model(parameter_info: pd.DataFrame) -> pd.DataFrame:\n",
        "    model_name = parameter_info.model.values[0]\n",
        "    model = LOADER_LOOKUP[model_name]()\n",
        "\n",
        "    def evaluate_model_for_parameters(parameters: pd.Series) -> List[Tuple[str, float]]:\n",
        "        result = list(\n",
        "            evaluate_model_latency(\n",
        "                model,\n",
        "                parameters[\"repeat\"],\n",
        "                parameters[\"prompt\"],\n",
        "                parameters[\"max_tokens\"],\n",
        "            )\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    parameter_info[\"results\"] = parameter_info.apply(\n",
        "        evaluate_model_for_parameters, axis=1\n",
        "    )\n",
        "    return parameter_info\n",
        "\n",
        "\n",
        "def load_and_evaluate_model(parameter_info: pd.DataFrame) -> pd.DataFrame:\n",
        "    try:\n",
        "        ref = _load_and_evaluate_model.remote(parameter_info)\n",
        "        return ray.get(ref)\n",
        "    except BaseException as e:\n",
        "        # Needed to prevent task retry after keyboard interrupt.\n",
        "        ray.cancel(ref, force=True)\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89UjLr4zJUXe"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "import openai\n",
        "\n",
        "\n",
        "def get_pipeline_wrapper(pipeline: Pipeline) -> Callable[[str, int], str]:\n",
        "    def wrapper(prompt: str, max_length: int) -> str:\n",
        "        result = pipeline(prompt, max_length=max_length)[0]['generated_text']\n",
        "        return result[len(prompt) :]  # Eliminate the prompt.\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def falcon7b_default_loader() -> Callable[[str, int], str]:\n",
        "    model = \"tiiuae/falcon-7b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    return get_pipeline_wrapper(\n",
        "        pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def falcon7b_quantized_loader() -> Callable[[str, int], str]:\n",
        "    model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, quantization_config=bnb_config, trust_remote_code=True\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "    return get_pipeline_wrapper(\n",
        "        pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def llama_loader() -> Callable[[str, int], str]:\n",
        "    return get_pipeline_wrapper(\n",
        "        pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"huggyllama/llama-7b\",  # Meta's version on HF is intentionally broken.\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def llama_cpp_loader() -> Callable[[str, int], str]:\n",
        "    llm = Llama(\n",
        "        model_path=\"/drive/MyDrive/Colab Datasets/ggml/ggml-model-f16.bin\",\n",
        "        n_gpu_layers=35,\n",
        "        n_ctx=2048,\n",
        "    )\n",
        "\n",
        "    def wrapper(prompt: str, max_length: int) -> str:\n",
        "        return llm(prompt, max_tokens=max_length, echo=False)[\"choices\"][0][\"text\"]\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def open_ai_loader() -> Callable[[str, int], str]:\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
        "    def invoke(prompt, max_length):\n",
        "        # Payload with the conversation messages\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        openai.api_key = \";)\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=messages,\n",
        "            max_tokens=max_length,\n",
        "        )\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    return invoke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2MaYAD3LBby"
      },
      "outputs": [],
      "source": [
        "PROMPT_LIBRARY = {\n",
        "    \"HOWDY\": \"Howdy! Tell me a bit about Louisiana.\",\n",
        "    \"EXTRACT\": 'Input JSON with key \"text\".\\n\\nYour job is to take text \"text\", extract all information conveyed by the text into a list of complete sentences, and provide a short title describing the content of the text.\\n\\nYour output should be a JSON with a key \"title\" pointing to the string title and a key \"info_list\" pointing to a list of strings representing the result of your job.\\n\\nInput:\\n        {\\n            \"text\": \"WASHINGTON (CNN)  -- A pair of tornadoes struck suburban Washington on Sunday, mangling trees and stripping siding off several homes, the National Weather Service confirmed. No injuries were immediately reported. The first tornado hit St. Charles, Maryland -- about 30 miles south of Washington -- just after 2 p.m. It uprooted several trees, many of which fell onto cars and homes. The strongest wind from that touchdown was 80 mph -- enough force to blow out windows. A second tornado followed about 30 minutes later outside Hyattsville, Maryland -- about 10 miles northeast of the capital. The high-speed winds, peaking at 100 mph, hit the George E. Peters Adventist School especially hard, tearing off a portion of the roof and flinging it and mounds of debris into the parking lot. A nearby construction trailer was also knocked over. E-mail to a friend .\"\\n        }\\n        ',\n",
        "    \"EXTRACT_LONG\": '\\n        Input JSON with key \"text\".\\n\\nYour job is to take text \"text\", extract all information conveyed by the text into a list of complete sentences, and provide a short title describing the content of the text.\\n\\nYour output should be a JSON with a key \"title\" pointing to the string title and a key \"info_list\" pointing to a list of strings representing the result of your job.\\n\\nInput:\\n        {\\n            \"text\": \"FARGO, North Dakota (CNN)  -- Forecasters issued flash flood warnings for Bismarck and surrounding areas Wednesday, as volunteers rushed to fill sandbags ahead of expected record floods in the flat state of North Dakota. Explosives are set off in the Missouri River on Wednesday to break up ice jams. Areas of three counties -- Morton, Emmons and Burleigh, which includes the North Dakota capital of Bismarck -- were under a flash flood warning until 12:30 p.m. CT (1:30 p.m. ET), the National Weather Service said. In an effort to alleviate the flooding, demolition crews blew up an ice jam Wednesday evening south of  Bismarck, according to CNN affiliate KXMB. Mayor John Warford said that water appeared to be moving more freely in the Missouri River after the explosives were set off, KXMB reported. The plan is make sure water continues flow through the river channel and not spread out over land. Ice jams in rivers have been a major factor in the flooding there. Most of the state, which endured a particularly harsh winter, remained under a flood warning Wednesday, with forecasters predicting possibly record flood levels on several rivers. Snow, which continued to fall Wednesday, complicated preparations, city officials said. \"I woke up this morning and looked outside, I guess I thought of the same thing everybody else did. ... [What] came to mind is what a revolting development this is,\" said Mark Voxland, the mayor of Moorhead, Minnesota, a city just outside of Fargo.  Watch flooded fields of snow » . More than 1,000 people were evacuated from an area near Bismarck on Tuesday night as the Missouri River flooded, Rick Robinson of the North Dakota Department of Emergency Services said Wednesday.  See a map of the affected area » . Emergency officials said they were particularly concerned about the Red River, which snakes through eastern portions of North and South Dakota and western Minnesota. The river is expected to crest between 39 and 41 feet in Fargo on Friday, according to Cecily Fong of the North Dakota Department of Emergency Services. The record for the Red River at Fargo was set in 1897 at 40.1 feet, according to Pat Slattery of the National Oceanic and Atmospheric Administration. The threat of flooding prompted authorities to ask for volunteers to fill sandbags either to build temporary dikes or to bolster existing ones. In some areas, even at 3:30 a.m., hundreds of volunteers packed into individual sandbagging centers, an organizer said.  See images of flooding, preparation » . \"There have been so many volunteers that we had to turn people away,\" said Ryan McEwan, a supervisor at one volunteer coordinating center. \"It is very busy. They are filling sandbags as fast as they can.\" As of late Tuesday, Fargo residents and out-of-town volunteers had filled more than 1 million sandbags out of the needed 2 million. Fargo Deputy Mayor Tim Mahoney said he hoped that goal would be met by Thursday. Fargo\\'s mayor, Dennis Walaker, said Wednesday that his city was about 95 percent prepared for the flooding, which is expected later in the week. \"I went and looked at the dikes this morning, and they\\'re significant, absolutely significant,\" he said in a briefing Wednesday morning. However, he said, \"We have some areas we need to shore up.\" Just south of Fargo, authorities said they had rescued several people in Oxbow, a town of about 238 people, after a residential dike gave way. In some places, water had reached halfway up residents\\' basement stairs, and in others, it had reached the main level of homes, Sgt. Gail Wichmann said. CNN\\'s Chris Welch contributed to this report.\"\\n        }\\n        {\"title\": \"Flash Flood Warning in North Dakota\", \"info_list\": [\"Flash flood warnings were issued for Bismarck and surrounding areas due to expected record floods in the flat state of North Dakota.\", \"Volunteers rushed to fill sandbags to prepare for the floods.\", \"Areas of three counties, including the North Dakota capital of Bismarck, were under a flash flood warning.\", \"Demolition crews blew up an ice jam south of Bismarck to alleviate the flooding.\", \"Most of the state remained under a flood warning, with possibly record flood levels predicted on several rivers.\", \"The Red River, which snakes through eastern portions of North and South Dakota and western Minnesota, was expected to crest between 39 and 41 feet in Fargo on Friday, threatening the area with flooding.\", \"Emergency officials asked for volunteers to fill sandbags to build temporary dikes or bolster existing ones.\", \"Residents and out-of-town volunteers had filled more than 1 million sandbags out of the needed 2 million as of late Tuesday.\", \"Fargo\\'s mayor reported that the city was about 95% prepared, but there were still areas that needed to be shored up.\", \"An ice jam in rivers was a significant factor in the flooding.\", \"Snowfall on Wednesday complicated preparations.\", \"Evacuations were conducted, and several people in Oxbow were rescued when a residential dike gave way.\"]}'\n",
        "}\n",
        "LOADER_LOOKUP = {\n",
        "    \"ybelkada/falcon-7b-sharded-bf16\": falcon7b_quantized_loader,\n",
        "    \"tiiuae/falcon-7b\": falcon7b_default_loader,\n",
        "    \"llama\": llama_loader,\n",
        "    \"llama_cpp\": llama_cpp_loader,\n",
        "    \"open_ai\": open_ai_loader,\n",
        "}\n",
        "EVALUATE_MODELS = [\n",
        "    # \"tiiuae/falcon-7b\",\n",
        "    # \"ybelkada/falcon-7b-sharded-bf16\",\n",
        "    \"open_ai\",\n",
        "    \"llama_cpp\",\n",
        "    # \"llama\",\n",
        "]\n",
        "# REPEAT = 5\n",
        "# PROMPT_NAMES = [\"HOWDY\", \"EXTRACT\"]\n",
        "# MAX_TOKENS = [64, 1024]\n",
        "REPEAT = 5\n",
        "PROMPT_NAMES = [\"HOWDY\", \"EXTRACT\", \"EXTRACT_LONG\", \"EXTRACT_LONG\"]\n",
        "MAX_TOKENS = [2**i for i in range(5, 12, 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QN0m3moH6YRc",
        "outputId": "0a7da372-c30e-4791-fc8c-5b40bbeb7378"
      },
      "outputs": [],
      "source": [
        "experiments = pd.DataFrame({\"model\": EVALUATE_MODELS, \"repeat\": REPEAT})\n",
        "parameters = pd.DataFrame(\n",
        "    itertools.product(PROMPT_NAMES, MAX_TOKENS), columns=[\"prompt_name\", \"max_tokens\"]\n",
        ")\n",
        "experiments = experiments.merge(parameters, how=\"cross\")\n",
        "experiments[\"prompt\"] = experiments[\"prompt_name\"].map(PROMPT_LIBRARY)\n",
        "experiments = experiments.loc[\n",
        "    experiments[\"prompt\"].str.len() <= (experiments[\"max_tokens\"] * 3.75)\n",
        "].reset_index(drop=True)\n",
        "\n",
        "experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ7RPlAO7G-2",
        "outputId": "f90c7ec9-a57b-4986-cb35-e37b322e99d9"
      },
      "outputs": [],
      "source": [
        "results = experiments.groupby([\"model\"], group_keys=False).apply(load_and_evaluate_model)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJJkypx99YMH"
      },
      "outputs": [],
      "source": [
        "def get_raw_per_character_seconds(r):\n",
        "    return sum([time / len(output) for output, time in r.results]) / len(r.results)\n",
        "\n",
        "\n",
        "def get_avg_output_length(r):\n",
        "    return sum([len(output) for output, _ in r.results]) / len(r.results)\n",
        "\n",
        "\n",
        "results[\"raw_per_character_latency_seconds\"] = results.apply(\n",
        "    get_raw_per_character_seconds, axis=1\n",
        ")\n",
        "results[\"get_avg_output_length_chars\"] = results.apply(get_avg_output_length, axis=1)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXN9jWBcBzrP"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi -L\n",
        "gpu_info = \"\\n\".join(gpu_info)\n",
        "\n",
        "results[\"gpu_type\"] = gpu_info.split(\":\")[1].split(\"(\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQboE8Pf_Hjn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "now = int(time.time())\n",
        "results.drop(columns=[\"results\"]).to_csv(\n",
        "    f\"/drive/MyDrive/Colab Datasets/inference_latency_analyzer_results/results_{now}.csv\",\n",
        "    index=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWijvdNV-Sg5"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = results.copy()\n",
        "\n",
        "df = df.rename(columns={\"get_avg_output_length_chars\": \"avg_output_length_in_chars\"})\n",
        "df[\"per_character_ms\"] = df[\"raw_per_character_latency_seconds\"] * 1000\n",
        "\n",
        "# Create a regression plot with grouping by the \"model\" column\n",
        "plot = sns.lmplot(x=\"avg_output_length_in_chars\",\n",
        "                 y=\"per_character_ms\",\n",
        "                 data=df,\n",
        "                 hue=\"model\", # This will create different colors/groups based on the 'model' column\n",
        "                 ci=None, # This disables the confidence interval shading\n",
        "                 legend=True)\n",
        "\n",
        "# Add a title\n",
        "plt.title(\"Average Character Latency vs Average Output Length by Model\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
